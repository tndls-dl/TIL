# ğŸ¤– Machine Learning

## âœï¸ DAY 03

## ğŸŒ³ğŸ¤ğŸ» Decision Tree & êµì°¨ê²€ì¦

## 1ï¸âƒ£ ë°ì´í„° ì¤€ë¹„

- ë°ì´í„°ì…‹: **wine.csv**
- íŠ¹ì§•(Features): `alcohol`, `sugar`, `pH`
- íƒ€ê¹ƒ(Target): `class`

```python
X = wine[['alcohol', 'sugar', 'pH']]
y = wine['class']

# Train / Test = 80:20
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

ğŸ‘‰ ì„ íƒ: **ìŠ¤ì¼€ì¼ë§** (Logistic Regressionìš©)

```python
ss = StandardScaler()
X_train_scaled = ss.fit_transform(X_train)
X_test_scaled = ss.transform(X_test)
```

---

## 2ï¸âƒ£ One-Hot Encoding (OHE)

- **ê°œë…**: ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆœì„œ/í¬ê¸° ì™œê³¡ ì—†ì´ 0/1 ë²¡í„°ë¡œ ë³€í™˜
- **ì˜ˆì‹œ**
    
    
    | Color | Label Encoding | One-Hot Encoding |
    | --- | --- | --- |
    | Red | 0 | [1, 0, 0] |
    | Blue | 1 | [0, 1, 0] |
    | Green | 2 | [0, 0, 1] |

âœ… ì¥ì : ìˆœì„œ ì™œê³¡ ë°©ì§€

âš  ë‹¨ì : ë²”ì£¼ ìˆ˜ ë§ìœ¼ë©´ ì°¨ì› í­ë°œ

```python
df_ohe = pd.get_dummies(df, columns=['Color'])
```

---

## 3ï¸âƒ£ ëª¨ë¸ í•™ìŠµ

### (1) Logistic Regression

```python
lr = LogisticRegression()
lr.fit(X_train_scaled, y_train)

print(lr.score(X_train_scaled, y_train))  # í›ˆë ¨ ì ìˆ˜
print(lr.score(X_test_scaled, y_test))    # í…ŒìŠ¤íŠ¸ ì ìˆ˜
print(lr.coef_, lr.intercept_)
```

- `.predict_proba()` â†’ í´ë˜ìŠ¤ë³„ í™•ë¥  ì¶œë ¥
- `.coef_, .intercept_` â†’ ì„ í˜• ê³„ìˆ˜ í™•ì¸

### (2) Decision Tree

```python
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

print(dt.score(X_train, y_train))  # í›ˆë ¨ ì ìˆ˜
print(dt.score(X_test, y_test))    # í…ŒìŠ¤íŠ¸ ì ìˆ˜
```

### ğŸŒ² íŠ¸ë¦¬ ì‹œê°í™”

```python
plt.figure(figsize=(12, 10))
plot_tree(dt, max_depth=2, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

### âš™ï¸ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ì„¤ëª… |
| --- | --- |
| `max_depth` | íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´ |
| `min_samples_split` | ë…¸ë“œ ë¶„í• ì„ ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ |
| `min_impurity_decrease` | ë…¸ë“œ ë¶„í•  ì‹œ ìµœì†Œ ë¶ˆìˆœë„ ê°ì†Œ |

---

## 4ï¸âƒ£ êµì°¨ê²€ì¦ (Cross Validation)

- **í›ˆë ¨ ë°ì´í„°ë§Œ ì‚¬ìš©**
- ëª¨ë¸ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€ìš©
- Test setì€ ë§ˆì§€ë§‰ 1ë²ˆë§Œ ì‚¬ìš©

```python
from sklearn.model_selection import cross_validate

scores = cross_validate(dt, X_train, y_train)
np.mean(scores['test_score'])
```

**ğŸ”¢ í´ë“œ ìˆ˜ ë³€ê²½ (ê¸°ë³¸ 5 â†’ 10)**

```python
from sklearn.model_selection import StratifiedKFold

splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_validate(dt, X_train, y_train, cv=splitter)
```

---

## 5ï¸âƒ£ GridSearchCV (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹)

- ë‹¨ê³„
    1. íƒìƒ‰í•  íŒŒë¼ë¯¸í„° ì •ì˜
    2. `GridSearchCV` ì‹¤í–‰ (ë‚´ë¶€ êµì°¨ê²€ì¦)
    3. ìµœì  ì¡°í•© í™•ì¸ â†’ `best_params_`, `best_score_`
    4. ìµœì¢… ëª¨ë¸ í•™ìŠµ â†’ `best_estimator_`

```python
params = {
    'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001),
    'max_depth': range(5, 20),
    'min_samples_split': range(2, 100, 10)
}

gs = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid=params,
    n_jobs=-1,
    cv=5
)

gs.fit(X_train, y_train)

print(gs.best_params_)
print(gs.best_score_)
```

ğŸ”¹ **best_score_ ê³„ì‚° ë°©ì‹**

- í›„ë³´ íŒŒë¼ë¯¸í„° ì¡°í•©ë³„ë¡œ K-Fold CV ìˆ˜í–‰
- foldë³„ ê²€ì¦ ì ìˆ˜ í‰ê·  = `mean_test_score`
- ìµœê³  í‰ê·  ì ìˆ˜ = `best_score_`

---

## 6ï¸âƒ£ ìµœì¢… ëª¨ë¸ í‰ê°€

- GridSearchë¡œ ì°¾ì€ ìµœì  ëª¨ë¸(`best_estimator_`)ì„ **í…ŒìŠ¤íŠ¸ì…‹ 1íšŒ í‰ê°€**

```python
dt_best = gs.best_estimator_
dt_best.score(X_test, y_test)
```

---

## 7ï¸âƒ£ ì‹œê°í™”

- ìµœì  íŠ¸ë¦¬ êµ¬ì¡° í™•ì¸

```python
plot_tree(dt_best, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
```

- íŠ¸ë¦¬ ê¹Šì´ / ë¶ˆìˆœë„ ì œí•œ â†’ ê³¼ì í•© ë°©ì§€
- OHE í›„ ë²”ì£¼í˜• íŠ¹ì„± ì‹œê°í™” â†’ ì…ë ¥ í™•ì¸ ê°€ëŠ¥

---

## 8ï¸âƒ£ í•µì‹¬ í¬ì¸íŠ¸

- **ë°ì´í„° ë¶„ë¦¬**: Train / Validation(CV) / Test
- **Decision Tree ê³¼ì í•©**: `max_depth`, `min_samples_split`, `min_impurity_decrease` ì¡°ì ˆ
- **GridSearchCV**: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ + êµì°¨ê²€ì¦ ë™ì‹œ ìˆ˜í–‰
- **OHE**: ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬ í•„ìˆ˜
- **ì‹œê°í™”**: íŠ¸ë¦¬ êµ¬ì¡°ì™€ Feature Importance íŒŒì•…ì— ë„ì›€