# ğŸ¤– Machine Learning

## **âœï¸ DAY 04**

## ğŸ²ğŸ¤ RandomizedSearchCV & ì•™ìƒë¸” í•™ìŠµ

## ğŸ² RandomizedSearchCV

### ğŸ“– ê°œë…

- **GridSearchCV**: ëª¨ë“  í›„ë³´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•© íƒìƒ‰ â†’ ì—°ì‚°ëŸ‰ â†‘
- **RandomizedSearchCV**: ì§€ì •ëœ ë²”ìœ„(distribution)ì—ì„œ ì„ì˜ë¡œ ìƒ˜í”Œë§ â†’ **ë¹ ë¥´ê³  íš¨ìœ¨ì **

```python
from scipy.stats import uniform, randint

# ì •ìˆ˜ ë²”ìœ„ ìƒ˜í”Œë§
print(randint(0, 10).rvs(10))

# ì‹¤ìˆ˜ ë²”ìœ„ ìƒ˜í”Œë§
print(uniform(0, 1).rvs(10))
```

### ğŸ“ í™œìš© ì˜ˆì‹œ

```python
params = {
    'min_impurity_decrease': uniform(0.0001, 0.001),
    'max_depth': randint(10, 50),
    'min_samples_split': randint(2, 25),
    'min_samples_leaf': randint(1, 25)
}

from sklearn.model_selection import RandomizedSearchCV
gs = RandomizedSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_distributions=params,
    n_iter=1000,
    n_jobs=-1,
    random_state=42
)

gs.fit(X_train, y_train)
print(gs.best_params_, gs.best_score_)
```

âœ… **`best_score_`** : ë‚´ë¶€ êµì°¨ê²€ì¦(CV)ì—ì„œ **ìµœì  íŒŒë¼ë¯¸í„° ì¡°í•©ì˜ í‰ê·  ì„±ëŠ¥**

---

## ğŸ¤ ì•™ìƒë¸” í•™ìŠµ (Ensemble Learning)

ğŸ“Œ **ì •í˜• ë°ì´í„°(í‘œ êµ¬ì¡° ë°ì´í„°)ì—ì„œ ê°€ì¥ ê°•ë ¥í•œ ì•Œê³ ë¦¬ì¦˜ ê³„ì—´**

ğŸ‘‰ ëŒ€ë¶€ë¶„ **íŠ¸ë¦¬ ê¸°ë°˜ (Decision Tree â†’ ì—¬ëŸ¬ ê°œ ì¡°í•©)**

### ğŸ”¹ Random Forest (ëœë¤ í¬ë ˆìŠ¤íŠ¸)

- ì—¬ëŸ¬ ê°œì˜ ê²°ì •íŠ¸ë¦¬ë¥¼ **ë³µì›ì¶”ì¶œ(bootstrap) ë°ì´í„°**ë¡œ í•™ìŠµ
- ì˜ˆì¸¡ ì‹œ:
    - **ë¶„ë¥˜**: ë‹¤ìˆ˜ê²° íˆ¬í‘œ
    - **íšŒê·€**: í‰ê· 
- ì¥ì : **ê³¼ì í•© ë°©ì§€ + ì•ˆì •ì  ì„±ëŠ¥**
- ë…¸ë“œ ë¶„í•  ì‹œ **íŠ¹ì„± ì¼ë¶€ë§Œ ê³ ë ¤** â†’ ë‹¤ì–‘í•œ íŠ¸ë¦¬ ìƒì„±

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_validate

rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, X_train, y_train, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

rf.fit(X_train, y_train)
print(rf.feature_importances_)
```

### âœ”ï¸ OOB (Out-Of-Bag) í‰ê°€

- ë³µì›ì¶”ì¶œ ì‹œ ì„ íƒë˜ì§€ ì•Šì€ ë°ì´í„°(OOB)ë¥¼ ê²€ì¦ì— í™œìš©

```python
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(X_train, y_train)
print(rf.oob_score_)
```

### âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```python
params = {
    'max_depth': randint(3, 50),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 20),
    'max_features': ['sqrt', 'log2', None],
    'n_estimators': randint(50, 300)
}

rs = RandomizedSearchCV(rf, params, n_iter=100, n_jobs=-1, random_state=42)
rs.fit(X_train, y_train)

print(rs.best_params_, rs.best_score_, rs.score(X_test, y_test))
```

### ğŸ”¹ Extra Trees (Extremely Randomized Trees)

- Random Forestì™€ ê±°ì˜ ìœ ì‚¬
- ì°¨ì´ì :
    - **ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§ X** â†’ ì „ì²´ ë°ì´í„° ì‚¬ìš©
    - **ë…¸ë“œ ë¶„í• : ìµœì  ë¶„í•  ëŒ€ì‹  ë¬´ì‘ìœ„ ë¶„í• **
- ì¥ì : ë¬´ì‘ìœ„ì„± â†‘ â†’ **ê³¼ì í•© ë°©ì§€**, **ë…¸ì´ì¦ˆ ë°ì´í„°ì—ì„œ ìœ ë¦¬**

```python
from sklearn.ensemble import ExtraTreesClassifier

et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, X_train, y_train, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

### ğŸ”¹ Gradient Boosting (GB)

- **ë¶€ìŠ¤íŒ…(Boosting)**: ì•½í•œ ëª¨ë¸(ì–•ì€ íŠ¸ë¦¬)ì„ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ â†’ ì´ì „ ëª¨ë¸ì˜ **ì˜¤ì°¨ ë³´ì™„**
- ë°©ì‹:
    1. ì²« ë²ˆì§¸ íŠ¸ë¦¬ í•™ìŠµ
    2. ì˜¤ì°¨(ì”ì°¨) ê³„ì‚°
    3. ì”ì°¨ ì˜ˆì¸¡í•˜ë„ë¡ ë‹¤ìŒ íŠ¸ë¦¬ í•™ìŠµ
    4. ë°˜ë³µ â†’ ì˜¤ì°¨ ì¤„ì—¬ê°
- ì¥ì : **ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥**
- ë‹¨ì : **í•™ìŠµ ì†ë„ ëŠë¦¼**, **í•˜ì´í¼íŒŒë¼ë¯¸í„° ë§ìŒ**

```python
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, X_train, y_train, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

### ğŸ”¹ HistGradientBoosting (HGB)

- **Gradient Boosting ê°œì„  ë²„ì „ (ì†ë„ â†‘)**
- ì…ë ¥ íŠ¹ì„±ì„ **256ê°œ êµ¬ê°„(bin)**ìœ¼ë¡œ ë‚˜ëˆ ì„œ ë¶„í•  í›„ë³´ ì¤„ì„ â†’ ë¹ ë¥¸ í•™ìŠµ
- **NaN(ê²°ì¸¡ì¹˜) ìë™ ì²˜ë¦¬ ê°€ëŠ¥**

```python
from sklearn.ensemble import HistGradientBoostingClassifier

hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, X_train, y_train, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

### ğŸ§¹ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²• ë¹„êµ

| ë°©ë²• | ì„¤ëª… | ì¥ì  | ë‹¨ì  |
| --- | --- | --- | --- |
| **Imputation** | í‰ê· /ì¤‘ì•™ê°’ ëŒ€ì²´, KNN, MICE ë“± | ëª¨ë¸ ë…ë¦½ì , ë„ë©”ì¸ ì§€ì‹ ë°˜ì˜ ê°€ëŠ¥ | ì˜ëª» ì±„ìš°ë©´ í¸í–¥, ê³„ì‚°ëŸ‰ â†‘ |
| **HGB ìë™ ì²˜ë¦¬** | NaN ê·¸ëŒ€ë¡œ ë„£ì–´ë„ í•™ìŠµ ê°€ëŠ¥ | ì „ì²˜ë¦¬ ë¶ˆí•„ìš”, ë¹ ë¦„ | ë‹¤ë¥¸ ëª¨ë¸ì—” ì ìš© ë¶ˆê°€, NaN ê³¼ì í•© ê°€ëŠ¥ |

### ğŸ”¹ XGBoost / LightGBM

- **Gradient Boosting ë¼ì´ë¸ŒëŸ¬ë¦¬** (ì†ë„Â·ì„±ëŠ¥ ìµœì í™”)
- Kaggle ë“± ëŒ€íšŒÂ·ì‹¤ë¬´ì—ì„œ **ê°€ì¥ ë§ì´ ì“°ì„**

```python
from xgboost import XGBClassifier
xgb = XGBClassifier(tree_method='hist', random_state=42)
xgb.fit(X_train, y_train)

from lightgbm import LGBMClassifier
lgb = LGBMClassifier(random_state=42)
lgb.fit(X_train, y_train)
```

---

## âœ¨ ì˜¤ëŠ˜ í•µì‹¬ ì •ë¦¬

âœ… **RandomizedSearchCV**

- í™•ë¥ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§ â†’ ë¹ ë¥¸ íƒìƒ‰
- `best_score_` = êµì°¨ê²€ì¦ í‰ê·  ì„±ëŠ¥

âœ… **ì•™ìƒë¸” í•™ìŠµ**

- **Random Forest**: ì•ˆì •ì , ê³¼ì í•© ë°©ì§€, OOB í™œìš© ê°€ëŠ¥
- **Extra Trees**: ë” ë¬´ì‘ìœ„, ë…¸ì´ì¦ˆ ë§ì€ ë°ì´í„°ì— ìœ ë¦¬
- **Gradient Boosting**: ì”ì°¨ í•™ìŠµ â†’ ì„±ëŠ¥ â†‘, ì†ë„ â†“
- **HistGradientBoosting**: ë¹ ë¦„, NaN ìë™ ì²˜ë¦¬
- **XGBoost / LightGBM**: ì‹¤ë¬´Â·ëŒ€íšŒ ìµœê°•