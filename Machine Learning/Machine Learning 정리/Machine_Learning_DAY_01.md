# 🤖 Machine Learning

## ✍️ DAY 01

## 🍎🍏 지도학습 vs 비지도학습

| 구분 | 특징 | 예시 |
| --- | --- | --- |
| 지도학습 (Supervised) | 입력(X) + 정답(y) → 모델 학습 | 분류(Classification): 도미/빙어  
회귀(Regression): 물고기 무게 |
| 비지도학습 (Unsupervised) | 정답 없음 → 패턴/구조 학습 | 군집(K-means), 차원축소(PCA), 이상치 탐지 |

🔑 핵심:

- 지도학습은 **분류(Classification)**와 **회귀(Regression)** 두 가지로 나뉨
- 반드시 **훈련 데이터 / 테스트 데이터 분리 → 일반화 성능 확인**

---

## 🧑‍🤝‍🧑 K-Nearest Neighbors (KNN)

- **아이디어**: 가까운 K개의 이웃을 보고 결과 결정

| 종류 | 동작 방식 |
| --- | --- |
| 분류 (Classification) | 다수결 투표 → 가장 많은 클래스 선택 |
| 회귀 (Regression) | 이웃들의 평균값을 사용해 예측 |

### 🔢 하이퍼파라미터 K

- K 작음 → 훈련 데이터에 민감 → **과대적합**
- K 큼 → 주변 데이터까지 포함 → **과소적합**

### ⚠️ 주의점

- **거리 기반 모델** → 특성 단위 다르면 **스케일링 필수** (StandardScaler, MinMaxScaler)

---

## 📈 회귀 (Regression)

### (1) 선형 회귀 (Linear Regression)

- 데이터를 직선으로 설명
- 식: `y = ax + b`
- 단순 모델 → **과소적합 위험**

### (2) 다항 회귀 (Polynomial Regression)

- 입력 특성을 **제곱, 세제곱** 등으로 확장 후 선형 회귀 적용
- 고차 다항식 → 복잡한 패턴 학습 가능
- 하지만 **과대적합 위험 ↑**

👆 Tip:

- `fit` 은 훈련 데이터 기준
- `transform` 은 테스트 데이터에만 적용 → **데이터 누수 방지**

---

## ⚖️ 규제 (Regularization)

| 종류 | 방식 | 특징 |
| --- | --- | --- |
| **릿지 (Ridge, L2)** | 계수 제곱 패널티 | 큰 계수 억제, 안정적, α↑ → 일반화 ↑ |
| **라쏘 (Lasso, L1)** | 계수 절댓값 패널티 | 일부 계수 0 → 변수 선택 효과, α↑ → 과소적합 |

🤝 공통 사항:

- **훈련/테스트 데이터 동일한 스케일링 필수**
- 규제 적용 → **과대적합 방지, 일반화 성능 향상**

---

## 💻 실습 포인트

- **KNN**: 거리 기반 → 스케일링 중요, K 값 조절 필요
- **선형/다항 회귀**: 단순 모델은 과소적합, 차수 높이면 과대적합 → 규제 필요
- **릿지/라쏘**: α 값 조절로 모델 복잡도/과적합 균형
- **항상 훈련/테스트 분리** → 과대적합/과소적합 확인 필수
- **다항 회귀 + 규제** → 과대적합 억제 가능

---

## 🗺️ 학습 흐름 (오늘 진행 순서)

1. KNN (분류/회귀) → 시각화, K 값 조절, 스케일링
2. 선형 회귀 → 단순 모델 → 과소적합 확인
3. 다항 회귀 → 차수 확장 → 과대적합 발생 가능성 확인
4. 규제 회귀 (Ridge/Lasso) → α 값 조절 → 과대적합 방지

**🎯 결론**

- 단순 모델 → 복잡 모델 → 규제 적용
- **훈련 점수 vs 테스트 점수 비교** → 모델의 **일반화 능력** 확인